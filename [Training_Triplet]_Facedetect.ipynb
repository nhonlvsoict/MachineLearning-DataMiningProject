{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "[Training_Triplet]_Facedetect.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nhonlvsoict/MachineLearning-DataMiningProject/blob/main/%5BTraining_Triplet%5D_Facedetect.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DiR7oJn4h4WE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1561eb18-2eb8-4d19-9699-45ddd6d41278"
      },
      "source": [
        "# Load the Drive helper and mount\n",
        "from google.colab import drive\n",
        "\n",
        "# This will prompt for authorization.\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--3iW5htq8TC",
        "outputId": "420f9dcd-700a-4e76-ac81-026c1fd9f53c"
      },
      "source": [
        "!pip uninstall -y keras-nightly\n",
        "!pip install tensorflow==1.15.0\n",
        "!pip install keras==2.1.5\n",
        "!pip install h5py==2.10.0\n",
        "!pip install tensorflow_gpu==1.15.0"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uninstalling keras-nightly-2.5.0.dev2021032900:\n",
            "  Successfully uninstalled keras-nightly-2.5.0.dev2021032900\n",
            "Collecting tensorflow==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/92/2b/e3af15221da9ff323521565fa3324b0d7c7c5b1d7a8ca66984c8d59cb0ce/tensorflow-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (412.3MB)\n",
            "\u001b[K     |████████████████████████████████| 412.3MB 39kB/s \n",
            "\u001b[?25hCollecting keras-applications>=1.0.8\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/e3/19762fdfc62877ae9102edf6342d71b28fbfd9dea3d2f96a882ce099b03f/Keras_Applications-1.0.8-py3-none-any.whl (50kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 6.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.34.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.36.2)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.2)\n",
            "Collecting gast==0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4e/35/11749bf99b2d4e3cceb4d55ca22590b0d7c2c62b9de38ac4a4a7f4687421/gast-0.2.2.tar.gz\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.19.5)\n",
            "Collecting tensorboard<1.16.0,>=1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/1e/e9/d3d747a97f7188f48aa5eda486907f3b345cd409f0a0850468ba867db246/tensorboard-1.15.0-py3-none-any.whl (3.8MB)\n",
            "\u001b[K     |████████████████████████████████| 3.8MB 38.7MB/s \n",
            "\u001b[?25hCollecting tensorflow-estimator==1.15.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/62/2ee9cd74c9fa2fa450877847ba560b260f5d0fb70ee0595203082dafcc9d/tensorflow_estimator-1.15.1-py2.py3-none-any.whl (503kB)\n",
            "\u001b[K     |████████████████████████████████| 512kB 32.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (3.12.4)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (1.12.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==1.15.0) (0.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow==1.15.0) (3.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (57.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.3.4)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow==1.15.0) (1.5.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (4.5.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow==1.15.0) (3.7.4.3)\n",
            "Building wheels for collected packages: gast\n",
            "  Building wheel for gast (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gast: filename=gast-0.2.2-cp37-none-any.whl size=7557 sha256=bb11db2ab261f423a10f24c0c63ffc4198221cd196847feaf7cbe6ee8fd62c4a\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/2e/7e/a1d4d4fcebe6c381f378ce7743a3ced3699feb89bcfbdadadd\n",
            "Successfully built gast\n",
            "\u001b[31mERROR: tensorflow-probability 0.12.1 has requirement gast>=0.3.2, but you'll have gast 0.2.2 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement tensorflow>=2.0.0, but you'll have tensorflow 1.15.0 which is incompatible.\u001b[0m\n",
            "Installing collected packages: keras-applications, gast, tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: gast 0.4.0\n",
            "    Uninstalling gast-0.4.0:\n",
            "      Successfully uninstalled gast-0.4.0\n",
            "  Found existing installation: tensorboard 2.5.0\n",
            "    Uninstalling tensorboard-2.5.0:\n",
            "      Successfully uninstalled tensorboard-2.5.0\n",
            "  Found existing installation: tensorflow-estimator 2.5.0\n",
            "    Uninstalling tensorflow-estimator-2.5.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.5.0\n",
            "  Found existing installation: tensorflow 2.5.0\n",
            "    Uninstalling tensorflow-2.5.0:\n",
            "      Successfully uninstalled tensorflow-2.5.0\n",
            "Successfully installed gast-0.2.2 keras-applications-1.0.8 tensorboard-1.15.0 tensorflow-1.15.0 tensorflow-estimator-1.15.1\n",
            "Collecting keras==2.1.5\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ba/65/e4aff762b8696ec0626a6654b1e73b396fcc8b7cc6b98d78a1bc53b85b48/Keras-2.1.5-py2.py3-none-any.whl (334kB)\n",
            "\u001b[K     |████████████████████████████████| 337kB 8.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from keras==2.1.5) (3.13)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.5) (1.4.1)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.5) (1.15.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.7/dist-packages (from keras==2.1.5) (1.19.5)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.4.3\n",
            "    Uninstalling Keras-2.4.3:\n",
            "      Successfully uninstalled Keras-2.4.3\n",
            "Successfully installed keras-2.1.5\n",
            "Collecting h5py==2.10.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/c0/abde58b837e066bca19a3f7332d9d0493521d7dd6b48248451a9e3fe2214/h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9MB)\n",
            "\u001b[K     |████████████████████████████████| 2.9MB 7.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.7 in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.19.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py==2.10.0) (1.15.0)\n",
            "Installing collected packages: h5py\n",
            "  Found existing installation: h5py 3.1.0\n",
            "    Uninstalling h5py-3.1.0:\n",
            "      Successfully uninstalled h5py-3.1.0\n",
            "Successfully installed h5py-2.10.0\n",
            "Collecting tensorflow_gpu==1.15.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bc/72/d06017379ad4760dc58781c765376ce4ba5dcf3c08d37032eeefbccf1c51/tensorflow_gpu-1.15.0-cp37-cp37m-manylinux2010_x86_64.whl (411.5MB)\n",
            "\u001b[K     |████████████████████████████████| 411.5MB 42kB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard<1.16.0,>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (1.1.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (1.34.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (1.19.5)\n",
            "Requirement already satisfied: tensorflow-estimator==1.15.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (1.15.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (0.36.2)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (0.2.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (3.12.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (3.3.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow_gpu==1.15.0) (0.12.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15.0) (3.3.4)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15.0) (57.0.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15.0) (1.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow_gpu==1.15.0) (2.10.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15.0) (4.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15.0) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<1.16.0,>=1.15.0->tensorflow_gpu==1.15.0) (3.4.1)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.15.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xjYxgZ8pioMe"
      },
      "source": [
        "import numpy as np\n",
        "import cv2"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ruYJq980nUVn"
      },
      "source": [
        "import pickle\n",
        "\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/X_train_triplet1.pkl', 'rb') as f:\n",
        "    X = pickle.load(f)\n",
        "    X = np.array(X)\n",
        "    X=X[2400:]\n",
        "#     X = np.expand_dims(X, axis=3)\n",
        "    X = X/255.\n",
        "with open('/content/drive/MyDrive/Colab Notebooks/y_train_triplet1.pkl', 'rb') as f:\n",
        "    y = pickle.load(f)\n",
        "    y = np.array(y)\n",
        "    y=y[2400:]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JJ7hU-sJ0XcO"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "import gc\n",
        "\n",
        "gc.collect()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uC0BtbaF9OvU"
      },
      "source": [
        "np.shape(X), np.shape(y)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ePUMUd7MDMxG"
      },
      "source": [
        "# import sklearn.preprocessing\n",
        "\n",
        "# label_binarizer = sklearn.preprocessing.LabelBinarizer()\n",
        "\n",
        "# label_binarizer.fit(range(4))\n",
        "\n",
        "# y = label_binarizer.transform(y)\n",
        "# y_val = label_binarizer.transform(y_val)\n",
        "\n",
        "# print(y.shape)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPYMxN2tvgF5"
      },
      "source": [
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras import applications"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d84U9ihiHbFi"
      },
      "source": [
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import backend as K\n",
        "import h5py\n",
        "import tensorflow.keras.applications.vgg16 as vgg16\n",
        "\n",
        "def convnet_model_():\n",
        "    vgg_model = vgg16.VGG16(weights=None, include_top=False, input_shape=(221, 221, 3))\n",
        "    x = vgg_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(4096, activation='relu')(x)\n",
        "    x = Dropout(0.6)(x)\n",
        "    x = Dense(4096, activation='relu')(x)\n",
        "    x = Dropout(0.6)(x)\n",
        "    x = Lambda(lambda x_: K.l2_normalize(x,axis=1))(x)\n",
        "#     x = Lambda(K.l2_normalize)(x)\n",
        "    convnet_model = Model(inputs=vgg_model.input, outputs=x)\n",
        "    return convnet_model\n",
        "\n",
        "def deep_rank_model():\n",
        "    convnet_model = convnet_model_()\n",
        "\n",
        "    first_input = Input(shape=(221, 221, 3))\n",
        "    first_conv = Conv2D(96, kernel_size=(8,8), strides=(16,16), padding='same')(first_input)\n",
        "    first_max = MaxPool2D(pool_size=(3,3), strides=(2,2), padding='same')(first_conv)\n",
        "    first_max = Flatten()(first_max)\n",
        "#     first_max = Lambda(K.l2_normalize)(first_max)\n",
        "    first_max = Lambda(lambda x: K.l2_normalize(x, axis=1))(first_max)\n",
        "\n",
        "    second_input = Input(shape=(221, 221, 3))\n",
        "    second_conv = Conv2D(96, kernel_size=(8,8), strides=(32,32), padding='same')(second_input)\n",
        "    second_max = MaxPool2D(pool_size=(7,7), strides=(4,4), padding='same')(second_conv)\n",
        "    second_max = Flatten()(second_max)\n",
        "    second_max = Lambda(lambda x: K.l2_normalize(x, axis=1))(second_max)\n",
        "#     second_max = Lambda(K.l2_normalize)(second_max)\n",
        "                       \n",
        "    merge_one = concatenate([first_max, second_max])\n",
        "    merge_two = concatenate([merge_one, convnet_model.output])\n",
        "    emb = Dense(4096)(merge_two)\n",
        "    emb = Dense(128)(emb)\n",
        "    l2_norm_final = Lambda(lambda x: K.l2_normalize(x, axis=1))(emb)\n",
        "#     l2_norm_final = Lambda(K.l2_normalize)(emb)\n",
        "                        \n",
        "    final_model = Model(inputs=[first_input, second_input, convnet_model.input], outputs=l2_norm_final)\n",
        "\n",
        "    return final_model"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVwwf38VHmWq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1e5f87d7-09fa-411a-c424-88b72d8c83a1"
      },
      "source": [
        "deep_rank_model = deep_rank_model()\n",
        "deep_rank_model.summary()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "WARNING:tensorflow:Large dropout rate: 0.6 (>0.5). In TensorFlow 2.x, dropout() uses dropout rate instead of keep_prob. Please ensure that this is intended.\n",
            "Model: \"model_1\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            [(None, 221, 221, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv1 (Conv2D)           (None, 221, 221, 64) 1792        input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "block1_conv2 (Conv2D)           (None, 221, 221, 64) 36928       block1_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block1_pool (MaxPooling2D)      (None, 110, 110, 64) 0           block1_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv1 (Conv2D)           (None, 110, 110, 128 73856       block1_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block2_conv2 (Conv2D)           (None, 110, 110, 128 147584      block2_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block2_pool (MaxPooling2D)      (None, 55, 55, 128)  0           block2_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv1 (Conv2D)           (None, 55, 55, 256)  295168      block2_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv2 (Conv2D)           (None, 55, 55, 256)  590080      block3_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_conv3 (Conv2D)           (None, 55, 55, 256)  590080      block3_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block3_pool (MaxPooling2D)      (None, 27, 27, 256)  0           block3_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv1 (Conv2D)           (None, 27, 27, 512)  1180160     block3_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv2 (Conv2D)           (None, 27, 27, 512)  2359808     block4_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_conv3 (Conv2D)           (None, 27, 27, 512)  2359808     block4_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block4_pool (MaxPooling2D)      (None, 13, 13, 512)  0           block4_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv1 (Conv2D)           (None, 13, 13, 512)  2359808     block4_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv2 (Conv2D)           (None, 13, 13, 512)  2359808     block5_conv1[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_conv3 (Conv2D)           (None, 13, 13, 512)  2359808     block5_conv2[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "block5_pool (MaxPooling2D)      (None, 6, 6, 512)    0           block5_conv3[0][0]               \n",
            "__________________________________________________________________________________________________\n",
            "input_2 (InputLayer)            [(None, 221, 221, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "input_3 (InputLayer)            [(None, 221, 221, 3) 0                                            \n",
            "__________________________________________________________________________________________________\n",
            "global_average_pooling2d (Globa (None, 512)          0           block5_pool[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "conv2d (Conv2D)                 (None, 14, 14, 96)   18528       input_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 7, 7, 96)     18528       input_3[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 4096)         2101248     global_average_pooling2d[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d (MaxPooling2D)    (None, 7, 7, 96)     0           conv2d[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 2, 2, 96)     0           conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "dropout (Dropout)               (None, 4096)         0           dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "flatten (Flatten)               (None, 4704)         0           max_pooling2d[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 384)          0           max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 4096)         16781312    dropout[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 4704)         0           flatten[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_2 (Lambda)               (None, 384)          0           flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dropout_1 (Dropout)             (None, 4096)         0           dense_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "concatenate (Concatenate)       (None, 5088)         0           lambda_1[0][0]                   \n",
            "                                                                 lambda_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "lambda (Lambda)                 (None, 4096)         0           dropout_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 9184)         0           concatenate[0][0]                \n",
            "                                                                 lambda[0][0]                     \n",
            "__________________________________________________________________________________________________\n",
            "dense_2 (Dense)                 (None, 4096)         37621760    concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_3 (Dense)                 (None, 128)          524416      dense_2[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "lambda_3 (Lambda)               (None, 128)          0           dense_3[0][0]                    \n",
            "==================================================================================================\n",
            "Total params: 71,780,480\n",
            "Trainable params: 71,780,480\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHY7XziPKu94"
      },
      "source": [
        "batch_size = 24\n",
        "\n",
        "_EPSILON = K.epsilon()\n",
        "def _loss_tensor(y_true, y_pred):\n",
        "    y_pred = K.clip(y_pred, _EPSILON, 1.0 - _EPSILON)\n",
        "    loss = 0.\n",
        "    g = 1.\n",
        "    for i in range(0, batch_size, 3):\n",
        "        try:\n",
        "            q_embedding = y_pred[i]\n",
        "            p_embedding = y_pred[i+1]\n",
        "            n_embedding = y_pred[i+2]\n",
        "            D_q_p = K.sqrt(K.sum((q_embedding - p_embedding)**2))\n",
        "            D_q_n = K.sqrt(K.sum((q_embedding - n_embedding)**2))\n",
        "            loss = loss + g + D_q_p - D_q_n\n",
        "        except:\n",
        "            continue\n",
        "    loss = loss/batch_size*3\n",
        "    return K.maximum(loss, 0)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2lcWgrgK2HH"
      },
      "source": [
        "deep_rank_model.compile(loss=_loss_tensor, optimizer=SGD(lr=0.001, momentum=0.9, nesterov=True))"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4AZD-UMIbjA"
      },
      "source": [
        "def image_batch_generator(images, labels, batch_size):\n",
        "    labels = np.array(labels)\n",
        "    while True:\n",
        "        batch_paths = np.random.choice(a = len(images), size = batch_size//3)\n",
        "        input_1 = []\n",
        "        \n",
        "        for i in batch_paths:\n",
        "            pos = np.where(labels == labels[i])[0]\n",
        "            neg = np.where(labels != labels[i])[0]\n",
        "            \n",
        "            j = np.random.choice(pos)\n",
        "            while j == i:\n",
        "                j = np.random.choice(pos)\n",
        "             \n",
        "            k = np.random.choice(neg)\n",
        "            while k == i:\n",
        "                k = np.random.choice(neg)\n",
        "            \n",
        "            input_1.append(images[i])\n",
        "            input_1.append(images[j])\n",
        "            input_1.append(images[k])\n",
        "\n",
        "        input_1 = np.array(input_1)\n",
        "        input = [input_1, input_1, input_1]\n",
        "        yield(input, np.zeros((batch_size, )))"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uNpgPSzSz_4g"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "filepath = '/content/drive/MyDrive/Colab Notebooks/triplet_weight1.hdf5'\n",
        "checkpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_weights_only=True, save_best_only=True, mode='min')\n",
        "callbacks_list = [checkpoint]"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEQ3O2pqMFf9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d5cb8319-5d24-4d27-b3d0-2bd4d002493f"
      },
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "\n",
        "print(\"Loading pre-trained weight\")\n",
        "deep_rank_model.load_weights('/content/drive/MyDrive/Colab Notebooks/triplet_weight1.hdf5')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading pre-trained weight\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QTwIM190EKg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7b6d9ef-b5e3-4e0a-d0b1-c36ab991445a"
      },
      "source": [
        "\n",
        "deep_rank_model.fit_generator(generator=image_batch_generator(X, y, batch_size),\n",
        "                   steps_per_epoch=len(X)//batch_size,\n",
        "                   epochs=2000,\n",
        "                   verbose=1,\n",
        "                   initial_epoch= 528,\n",
        "                   callbacks=callbacks_list)\n",
        "     "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 529/2000\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "248/249 [============================>.] - ETA: 0s - loss: 0.2983\n",
            "Epoch 00529: loss improved from inf to 0.29765, saving model to /content/drive/MyDrive/Colab Notebooks/triplet_weight1.hdf5\n",
            "249/249 [==============================] - 111s 447ms/step - loss: 0.2977\n",
            "Epoch 530/2000\n",
            "248/249 [============================>.] - ETA: 0s - loss: 0.2822\n",
            "Epoch 00530: loss improved from 0.29765 to 0.28346, saving model to /content/drive/MyDrive/Colab Notebooks/triplet_weight1.hdf5\n",
            "249/249 [==============================] - 102s 411ms/step - loss: 0.2835\n",
            "Epoch 531/2000\n",
            "248/249 [============================>.] - ETA: 0s - loss: 0.2759\n",
            "Epoch 00531: loss improved from 0.28346 to 0.27541, saving model to /content/drive/MyDrive/Colab Notebooks/triplet_weight1.hdf5\n",
            "249/249 [==============================] - 104s 419ms/step - loss: 0.2754\n",
            "Epoch 532/2000\n",
            "248/249 [============================>.] - ETA: 0s - loss: 0.2778\n",
            "Epoch 00532: loss did not improve from 0.27541\n",
            "249/249 [==============================] - 102s 409ms/step - loss: 0.2786\n",
            "Epoch 533/2000\n",
            "248/249 [============================>.] - ETA: 0s - loss: 0.2898\n",
            "Epoch 00533: loss did not improve from 0.27541\n",
            "249/249 [==============================] - 102s 408ms/step - loss: 0.2892\n",
            "Epoch 534/2000\n",
            "248/249 [============================>.] - ETA: 0s - loss: 0.2823\n",
            "Epoch 00534: loss did not improve from 0.27541\n",
            "249/249 [==============================] - 102s 408ms/step - loss: 0.2821\n",
            "Epoch 535/2000\n",
            "248/249 [============================>.] - ETA: 0s - loss: 0.2843\n",
            "Epoch 00535: loss did not improve from 0.27541\n",
            "249/249 [==============================] - 102s 408ms/step - loss: 0.2846\n",
            "Epoch 536/2000\n",
            "248/249 [============================>.] - ETA: 0s - loss: 0.2840\n",
            "Epoch 00536: loss did not improve from 0.27541\n",
            "249/249 [==============================] - 102s 409ms/step - loss: 0.2838\n",
            "Epoch 537/2000\n",
            "248/249 [============================>.] - ETA: 0s - loss: 0.2682\n",
            "Epoch 00537: loss improved from 0.27541 to 0.26810, saving model to /content/drive/MyDrive/Colab Notebooks/triplet_weight1.hdf5\n",
            "249/249 [==============================] - 105s 421ms/step - loss: 0.2681\n",
            "Epoch 538/2000\n",
            "248/249 [============================>.] - ETA: 0s - loss: 0.2920\n",
            "Epoch 00538: loss did not improve from 0.26810\n",
            "249/249 [==============================] - 102s 408ms/step - loss: 0.2917\n",
            "Epoch 539/2000\n",
            "248/249 [============================>.] - ETA: 0s - loss: 0.2843\n",
            "Epoch 00539: loss did not improve from 0.26810\n",
            "249/249 [==============================] - 102s 408ms/step - loss: 0.2840\n",
            "Epoch 540/2000\n",
            "248/249 [============================>.] - ETA: 0s - loss: 0.2749\n",
            "Epoch 00540: loss did not improve from 0.26810\n",
            "249/249 [==============================] - 102s 408ms/step - loss: 0.2748\n",
            "Epoch 541/2000\n",
            "248/249 [============================>.] - ETA: 0s - loss: 0.2973\n",
            "Epoch 00541: loss did not improve from 0.26810\n",
            "249/249 [==============================] - 102s 408ms/step - loss: 0.2962\n",
            "Epoch 542/2000\n",
            "248/249 [============================>.] - ETA: 0s - loss: 0.2934\n",
            "Epoch 00542: loss did not improve from 0.26810\n",
            "249/249 [==============================] - 102s 408ms/step - loss: 0.2941\n",
            "Epoch 543/2000\n",
            "233/249 [===========================>..] - ETA: 6s - loss: 0.2822"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vvHpYahp36C-"
      },
      "source": [
        "from tqdm import tqdm\n",
        "    embs128 = []\n",
        "    for x in tqdm(X):\n",
        "        image = x/255\n",
        "        image = np.expand_dims(image, axis=0)\n",
        "        emb128 = deep_rank_model.predict([image, image, image])\n",
        "        embs128.append(emb128[0])\n",
        "        del image\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VMtr9jyVpgva"
      },
      "source": [
        "with open('/content/drive/MyDrive/Colab Notebooks/name_map1.pkl', 'rb') as f:\n",
        "    names = pickle.load(f)\n",
        "    names = np.array(y)\n",
        "    names=names[2400:]\n",
        "np.shape(names)    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kmyLh-S8yj2_"
      },
      "source": [
        "import time\n",
        "import dlib\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GWhjg9ftpQAb"
      },
      "source": [
        "\n",
        "image = cv2.imread('/content/drive/MyDrive/Colab Notebooks/lfw/Zico/Zico_0002.jpg')\n",
        "\n",
        "\n",
        "# Khai báo việc sử dụng các hàm của dlib\n",
        "hog_face_detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "faces_hog = hog_face_detector(image, 1)\n",
        "\n",
        "for face in faces_hog:\n",
        "    x = face.left()\n",
        "    y = face.top()\n",
        "    w = face.right() - x\n",
        "    h = face.bottom() - y\n",
        "    frame = image[y:y+h, x:x+w]\n",
        "    frame = cv2.resize(frame, (221, 221))\n",
        "    frame = frame /255.\n",
        "    frame = np.expand_dims(frame, axis=0)\n",
        "    emb128 = deep_rank_model.predict([frame, frame, frame])\n",
        "    \n",
        "    minimum = 99999\n",
        "    person = -1\n",
        "    for k, e in enumerate(embs128):\n",
        "        #Euler distance\n",
        "        dist = np.linalg.norm(emb128-e)\n",
        "        if dist < minimum:\n",
        "            minimum = dist\n",
        "            person = k\n",
        "    cv2.rectangle(image, (x,y), (x+w,y+h), (0,255,0), 2)\n",
        "    cv2.putText(image, str(name[person]), (x - 10, y - 10),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2BOYb3cuyns6"
      },
      "source": [
        "\n",
        "image = cv2.imread('/content/drive/MyDrive/Colab Notebooks/lfw/Yoko_Ono/Yoko_Ono_0002.jpg')\n",
        "\n",
        "\n",
        "# Khai báo việc sử dụng các hàm của dlib\n",
        "hog_face_detector = dlib.get_frontal_face_detector()\n",
        "\n",
        "faces_hog = hog_face_detector(image, 1)\n",
        "\n",
        "for face in faces_hog:\n",
        "    x = face.left()\n",
        "    y = face.top()\n",
        "    w = face.right() - x\n",
        "    h = face.bottom() - y\n",
        "    frame = image[y:y+h, x:x+w]\n",
        "    frame = cv2.resize(frame, (221, 221))\n",
        "    frame = frame /255.\n",
        "    frame = np.expand_dims(frame, axis=0)\n",
        "    emb128 = deep_rank_model.predict([frame, frame, frame])\n",
        "    minimum = 99999\n",
        "    person = -1\n",
        "    for k, e in enumerate(embs128):\n",
        "        #Euler distance\n",
        "        dist = np.linalg.norm(emb128-e)\n",
        "        if dist < minimum:\n",
        "            minimum = dist\n",
        "            person = k\n",
        "            # print(dist)\n",
        "    print(person)\n",
        "    cv2.rectangle(image, (x,y), (x+w,y+h), (0,255,0), 2)\n",
        "    cv2.putText(image, str(names[person]), (x - 10, y - 10),\n",
        "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
        "cv2_imshow(image)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v9b3BGS-tIcf"
      },
      "source": [
        "np.shape(names)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}